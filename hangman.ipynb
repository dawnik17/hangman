{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['aaa', 'aaaaaa', 'aaas', 'aachen', 'aaee'], 227300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [x.strip(\"\\n\") for x in open(\"words.txt\", \"r\")]\n",
    "train[:5], len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_ratio = 0.8\n",
    "random.shuffle(train)\n",
    "test_len = int((1 - split_ratio) * len(train))\n",
    "\n",
    "test = train[:test_len]\n",
    "train = train[test_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181841, 45459)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def comb(word):\n",
    "    chars = list(set(word))\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for i in range(len(chars)): \n",
    "        output.extend(list(combinations(chars, i+1)))\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for characters in output:\n",
    "        aug = word\n",
    "        for character in characters:\n",
    "            aug = aug.replace(character, \"_\")\n",
    "            \n",
    "        result.append([aug, word])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181841/181841 [03:12<00:00, 943.16it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['_rmoni_c', 'armoniac'],\n",
       " ['armon_ac', 'armoniac'],\n",
       " ['arm_niac', 'armoniac'],\n",
       " ['a_moniac', 'armoniac'],\n",
       " ['armo_iac', 'armoniac']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for word in tqdm(train):\n",
    "    train_data.extend(comb(word))\n",
    "    \n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45459/45459 [01:00<00:00, 757.49it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['d_arch_', 'dyarchy'],\n",
       " ['dy_rchy', 'dyarchy'],\n",
       " ['_yarchy', 'dyarchy'],\n",
       " ['dyarc_y', 'dyarchy'],\n",
       " ['dya_chy', 'dyarchy']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = []\n",
    "\n",
    "for word in tqdm(test):\n",
    "    test_data.extend(comb(word))\n",
    "    \n",
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80362015, 20363467)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['armoniac', 'ependytes', 'supertemptation', 'needfully']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181841/181841 [00:00<00:00, 889004.40it/s]\n",
      "100%|██████████| 45459/45459 [00:00<00:00, 803377.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for d in tqdm(train):\n",
    "    max_len = max(max_len, len(d))\n",
    "    \n",
    "for d in tqdm(test):\n",
    "    max_len = max(max_len, len(d))\n",
    "    \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2index(data=[]):\n",
    "    data = set([char for word in data for char in word])\n",
    "\n",
    "    char2idx = {c: i + 2 for i, c in enumerate(data)}\n",
    "    char2idx.update({\"<PAD>\": 0, \"_\": 1})\n",
    "\n",
    "    idx2char = {v: k for k, v in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "\n",
    "char2idx, idx2char = char2index(train+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'j': 2,\n",
       " 'z': 3,\n",
       " 'o': 4,\n",
       " 'q': 5,\n",
       " 'd': 6,\n",
       " 'r': 7,\n",
       " 'n': 8,\n",
       " 'u': 9,\n",
       " 'e': 10,\n",
       " 'y': 11,\n",
       " 'b': 12,\n",
       " 'a': 13,\n",
       " 'k': 14,\n",
       " 'v': 15,\n",
       " 'l': 16,\n",
       " 'g': 17,\n",
       " 'i': 18,\n",
       " 'c': 19,\n",
       " 'f': 20,\n",
       " 't': 21,\n",
       " 's': 22,\n",
       " 'p': 23,\n",
       " 'h': 24,\n",
       " 'x': 25,\n",
       " 'w': 26,\n",
       " 'm': 27,\n",
       " '<PAD>': 0,\n",
       " '_': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config, if exists\n",
    "config = None\n",
    "folder_path = './'\n",
    "\n",
    "if os.path.isfile(folder_path + 'checkpoint__/config.pkl'):\n",
    "    print('loading the config file..')\n",
    "    with open(folder_path + 'checkpoint__/config.pkl', 'rb') as fp:\n",
    "        config = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_ua5GpThlc1d"
   },
   "outputs": [],
   "source": [
    "class CDataset(Dataset):\n",
    "    def __init__(self, data, config=None):\n",
    "        self.data = data\n",
    "        self.nclass = len(char2idx)\n",
    "        \n",
    "        if config is None:\n",
    "            self.max_len = max_len + 1\n",
    "            self.char2idx, self.idx2char = char2idx, idx2char\n",
    "\n",
    "        else:\n",
    "            self.max_len = config[\"max_len\"]\n",
    "            self.char2idx = config[\"char2idx\"]\n",
    "            self.idx2char = {v: k for k, v in self.char2idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        inword = [self.char2idx[char] for char in data[0]]\n",
    "        \n",
    "        # outword\n",
    "        outword = [self.char2idx[char] for char in data[1]]\n",
    "\n",
    "        # word input padding\n",
    "        inword += [self.char2idx[\"<PAD>\"]] * (self.max_len - len(inword))\n",
    "        outword += [self.char2idx[\"<PAD>\"]] * (self.max_len - len(outword))\n",
    "        \n",
    "        return torch.LongTensor(inword), torch.LongTensor(outword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pJSe9i1tD118"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "cdata_train = CDataset(train_data, config)\n",
    "cdata_test = CDataset(test_data, config)\n",
    "\n",
    "# train dataloader\n",
    "train_dataloader_args =  dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "trainloader = DataLoader(cdata_train, pin_memory=True, **train_dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_dataloader_args =  dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "testloader = DataLoader(cdata_test, pin_memory=True, **test_dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "K6Zu8LCalc1d"
   },
   "outputs": [],
   "source": [
    "# save config\n",
    "if not os.path.isfile(folder_path + 'checkpoint__/config.pkl') :\n",
    "    config = dict()\n",
    "    \n",
    "    # aug_avail and word_percentages are the training data configuration and rest are model configuration\n",
    "    config.update({'max_len':cdata_train.max_len, \n",
    "                   'char2idx':cdata_train.char2idx})\n",
    "    \n",
    "    with open(folder_path + 'checkpoint__/config.pkl', 'wb') as fp:\n",
    "        pickle.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13,  7, 27,  1,  8, 18, 13,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([13,  7, 27,  4,  8, 18, 13, 19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdata_train.__getitem__(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "EMMo0sBhkTR1"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dimension):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dimension, padding_idx=0)\n",
    "        self.dimension = dimension\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        return self.embedding(input_vec) * math.sqrt(self.dimension)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, dimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        positional_enc = torch.zeros(max_seq_len, dimension)\n",
    "\n",
    "        den = torch.pow(\n",
    "            10000, torch.div(torch.arange(0, dimension / 2) * 2, float(dimension))\n",
    "        )\n",
    "        num = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "\n",
    "        positional_enc[:, 0::2], positional_enc[:, 1::2] = (\n",
    "            torch.sin(num / den),\n",
    "            torch.cos(num / den),\n",
    "        )\n",
    "        positional_enc = positional_enc.unsqueeze(0)\n",
    "        self.register_buffer(\"positional_enc\", positional_enc)\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        seq_len = input_vec.size(1)\n",
    "        return self.dropout(input_vec + Variable(self.positional_enc[:, :seq_len]))\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, dimension, heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dimension = dimension\n",
    "        self.queryl = nn.Linear(dimension, dimension)\n",
    "        self.keyl = nn.Linear(dimension, dimension)\n",
    "        self.valuel = nn.Linear(dimension, dimension)\n",
    "        self.outl = nn.Linear(dimension, dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        assert self.dimension == query.size(-1)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.queryl(query)\n",
    "        key = self.keyl(key)\n",
    "        value = self.valuel(value)\n",
    "\n",
    "        query = query.view(\n",
    "            batch_size, -1, self.heads, query.size(-1) // self.heads\n",
    "        ).transpose(1, 2)\n",
    "        key = key.view(\n",
    "            batch_size, -1, self.heads, key.size(-1) // self.heads\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        value = value.view(\n",
    "            batch_size, -1, self.heads, value.size(-1) // self.heads\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "\n",
    "        attn = self.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "\n",
    "        concat = attn.transpose(1, 2).reshape(\n",
    "            batch_size, -1, query.size(-1) * self.heads\n",
    "        )\n",
    "\n",
    "        return self.outl(concat)\n",
    "\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        qk = torch.div(\n",
    "            torch.matmul(query, key.transpose(-2, -1)), math.sqrt(query.size(-1))\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            qk = qk.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        qk = nn.Softmax(dim=-1)(qk)\n",
    "        qk = self.dropout(qk) if dropout is not None else qk\n",
    "        return torch.matmul(qk, value)\n",
    "\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, dimension, dff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.l = nn.Linear(dimension, dff)\n",
    "        self.out = nn.Linear(dff, dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        return self.out(self.dropout(F.gelu(self.l(input_vec))))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dimension, delta=1e-6):\n",
    "        super().__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(dimension))\n",
    "        self.bias = nn.Parameter(torch.zeros(dimension))\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        mean = torch.mean(input_vec, dim=-1, keepdim=True)\n",
    "        std = torch.std(input_vec, dim=-1, keepdim=True) + self.delta\n",
    "        return (self.gain / std) * (input_vec - mean) + self.bias\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_vec, sublayer):\n",
    "        return input_vec + self.dropout(sublayer(self.norm(input_vec)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dimension, head=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(dimension, head, dropout)\n",
    "        self.ffnn = FeedForwardNet(dimension, dropout=dropout)\n",
    "        self.resconn1 = ResidualConnection(dimension, dropout)\n",
    "        self.resconn2 = ResidualConnection(dimension, dropout)\n",
    "\n",
    "        self.norm = LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_vec, mask=None):\n",
    "        attn = self.resconn1(input_vec, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.resconn2(attn, self.ffnn), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, number_of_layers, head, max_seq_len, dimension, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(vocab_size, dimension)\n",
    "        self.penc = PositionalEncoding(max_seq_len, dimension, dropout)\n",
    "        self.enclays = nn.ModuleList(\n",
    "            [\n",
    "                copy.deepcopy(EncoderLayer(dimension, head, dropout))\n",
    "                for _ in range(number_of_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm(dimension)\n",
    "\n",
    "    def forward(self, input_vec, mask=None):\n",
    "        emb = self.emb(input_vec)\n",
    "        emb = self.penc(emb)\n",
    "\n",
    "        for layer in self.enclays:\n",
    "            emb, _ = layer(emb, mask)\n",
    "            \n",
    "        emb = self.norm(emb)\n",
    "        return emb\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        envocab_size,\n",
    "        devocab_size,\n",
    "        max_seq_len,\n",
    "        head,\n",
    "        number_of_layers,\n",
    "        dimension,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            envocab_size, number_of_layers, head, max_seq_len, dimension, dropout\n",
    "        )\n",
    "        self.ffnn = nn.Linear(dimension, devocab_size)\n",
    "\n",
    "    def forward(self, enc_input_vec, encmask=None):\n",
    "        encout = self.encoder(enc_input_vec, encmask)\n",
    "        return self.ffnn(encout)\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, device='cpu', pad=0):\n",
    "        self.src = src\n",
    "\n",
    "        src = (src!=pad).int()\n",
    "        self.src_mask = (src != pad).unsqueeze(1)\n",
    "\n",
    "class CustomAdam:\n",
    "    def __init__(self, dimension, optimizer, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.step_num = 0\n",
    "        self.dimension = dimension\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.rate()\n",
    "\n",
    "        for pg in self.optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self):\n",
    "        return self.dimension ** (-0.5) * min(\n",
    "            self.step_num ** (-0.5), self.step_num * self.warmup_steps ** (-1.5)\n",
    "        )\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_index, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        prediction = prediction.contiguous().view(-1, prediction.size(-1))\n",
    "        target = target.contiguous().view(-1)\n",
    "\n",
    "        one_hot_target = torch.nn.functional.one_hot(target, num_classes=prediction.size(-1))\n",
    "        one_hot_target[:, self.pad_index] = 0\n",
    "        one_hot_target.masked_fill_((target == self.pad_index).unsqueeze(1), 0)\n",
    "\n",
    "        return F.cross_entropy(prediction, one_hot_target.float())\n",
    "    \n",
    "        \n",
    "def init_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "F1RXGqCSpzq0"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = Transformer(envocab_size=len(config[\"char2idx\"]),\n",
    "                    devocab_size=len(config[\"char2idx\"]),\n",
    "                    max_seq_len=config[\"max_len\"],\n",
    "                    head=8,\n",
    "                    number_of_layers=3,\n",
    "                    dimension=64,\n",
    "                    dropout=0.1).to(device)\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vM_1ePjWlc1e",
    "outputId": "3906e81b-31d5-47a0-d602-e8fb02d4c4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights initialised..\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(folder_path + 'checkpoint__/model.pt'):\n",
    "    model.load_state_dict(torch.load(folder_path + 'checkpoint__/model.pt'), strict=False)\n",
    "    print('loading the model to train further..')\n",
    "    \n",
    "else:\n",
    "    model.apply(init_weights)\n",
    "    print('model weights initialised..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqCXKO5Mp4-R",
    "outputId": "f830bc4e-c384-4d32-9daa-db4773bc3930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 104,540 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "NOjCwhJ5p6yf"
   },
   "outputs": [],
   "source": [
    "optimizer = CustomAdam(\n",
    "        dimension=64,\n",
    "        warmup_steps=400,\n",
    "        optimizer=torch.optim.Adam(\n",
    "            model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "HGxSMwiup9Ws"
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(vocab_size=len(config[\"char2idx\"]), pad_index=0, alpha=0.1)\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "SGy0aaywp-t3"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, device, trainloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    \n",
    "    iterator = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    pbar = tqdm(trainloader)\n",
    "\n",
    "    for i, (src, trg) in enumerate(pbar, 0):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        batcher = Batch(src, device, 0)\n",
    "\n",
    "        output = model(batcher.src, batcher.src_mask)\n",
    "        # print(output.shape, trg.shape)\n",
    "        loss = criterion(output, trg)\n",
    "        # break\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.optimizer.zero_grad()\n",
    "\n",
    "        pred = output.argmax(-1)\n",
    "        trg[trg == 0] = -1\n",
    "\n",
    "        correct += pred.eq(trg.view_as(pred)).sum().item()\n",
    "        \n",
    "        trg[trg == -1] = 0\n",
    "        processed += torch.count_nonzero(trg).item()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # tqdm writing\n",
    "        pbar.set_description(\n",
    "            desc=\"Train Epoch - {epoch}, Mini Batch - {batch}, Train Loss - {loss}, Train Accuracy - {accuracy}\".format(\n",
    "                epoch=epoch + 1,\n",
    "                batch=i + 1,\n",
    "                loss=round(running_loss / (i+1), 4),\n",
    "                accuracy=round(100 * correct / processed, 4)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "kONJrqZqqAXs"
   },
   "outputs": [],
   "source": [
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "\n",
    "    iterator = 0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    pbar = tqdm(testloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(pbar, 0):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            batcher = Batch(src, device, 0)\n",
    "\n",
    "            output = model(batcher.src, batcher.src_mask)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(-1)\n",
    "            trg[trg == 0] = -1\n",
    "            \n",
    "            correct += pred.eq(trg.view_as(pred)).sum().item()\n",
    "            \n",
    "            trg[trg == -1] = 0\n",
    "            processed += torch.count_nonzero(trg).item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                desc=\"Test Loss - {loss}, Test Accuracy - {accuracy}\".format(\n",
    "                    loss=round(test_loss / (i+1), 4),\n",
    "                    accuracy=round(100 * correct / processed, 4)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch - 1, Mini Batch - 8037, Train Loss - 0.3643, Train Accuracy - 72.5317: 100%|██████████| 8037/8037 [1:29:57<00:00,  1.49it/s]  \n",
      "Test Loss - 0.341, Test Accuracy - 74.4724: 100%|██████████| 2037/2037 [17:13<00:00,  1.97it/s]   \n",
      "Train Epoch - 2, Mini Batch - 8037, Train Loss - 0.3642, Train Accuracy - 72.5402: 100%|██████████| 8037/8037 [1:25:24<00:00,  1.57it/s]  \n",
      "Test Loss - 0.3404, Test Accuracy - 74.5017: 100%|██████████| 2037/2037 [16:33<00:00,  2.05it/s] \n",
      "Train Epoch - 3, Mini Batch - 8037, Train Loss - 0.3641, Train Accuracy - 72.5457: 100%|██████████| 8037/8037 [1:21:44<00:00,  1.64it/s]  \n",
      "Test Loss - 0.3405, Test Accuracy - 74.5113: 100%|██████████| 2037/2037 [16:44<00:00,  2.03it/s]  \n",
      "Train Epoch - 4, Mini Batch - 8037, Train Loss - 0.364, Train Accuracy - 72.5515: 100%|██████████| 8037/8037 [1:22:30<00:00,  1.62it/s]  \n",
      "Test Loss - 0.3399, Test Accuracy - 74.527: 100%|██████████| 2037/2037 [16:31<00:00,  2.05it/s]  \n",
      "Train Epoch - 5, Mini Batch - 8037, Train Loss - 0.364, Train Accuracy - 72.5564: 100%|██████████| 8037/8037 [1:21:07<00:00,  1.65it/s]  \n",
      "Test Loss - 0.3403, Test Accuracy - 74.5153: 100%|██████████| 2037/2037 [16:49<00:00,  2.02it/s]  \n",
      "Train Epoch - 6, Mini Batch - 8037, Train Loss - 0.3639, Train Accuracy - 72.5634: 100%|██████████| 8037/8037 [1:21:49<00:00,  1.64it/s]  \n",
      "Test Loss - 0.3404, Test Accuracy - 74.5178: 100%|██████████| 2037/2037 [16:43<00:00,  2.03it/s]  \n",
      "Train Epoch - 7, Mini Batch - 8037, Train Loss - 0.3638, Train Accuracy - 72.5673: 100%|██████████| 8037/8037 [1:22:53<00:00,  1.62it/s]  \n",
      "Test Loss - 0.34, Test Accuracy - 74.5323: 100%|██████████| 2037/2037 [16:40<00:00,  2.04it/s]    \n",
      "Train Epoch - 8, Mini Batch - 8037, Train Loss - 0.3638, Train Accuracy - 72.5714: 100%|██████████| 8037/8037 [1:16:28<00:00,  1.75it/s]  \n",
      "Test Loss - 0.3399, Test Accuracy - 74.5377:  65%|██████▌   | 1329/2037 [09:57<04:39,  2.54it/s] "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, model, device, trainloader, optimizer)\n",
    "    test_loss = test(model, device, testloader)\n",
    "    \n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.optimizer.state_dict(),\n",
    "        },\n",
    "        \"./checkpoint/model.pt\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "test_case.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
